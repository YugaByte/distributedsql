module.exports = [
  [
    {
      "id": "m0wo0x",
      "speakers": ["KarthikRanganathan"],        
      "talk": "Introduction to YugabyteDB - Design and Architecture",
      "time": "10 - 10.30 am",        
      "start": "2020-09-15T10:00:00-07:00",
      "end": "2020-09-15T10:30:00-07:00",
      "description": "This workshop will introduce the architecture along with the basic concepts of YugabyteDB, a distributed SQL database. Learn about how data is distributed and replicated across a cluster of nodes to ensure high availability even on failures, how to scale out the cluster by simply adding nodes - without compromising ACID semantics and data integrity."
    },
    {
      "id": "to8g75",
      "speakers": ["NikhilChandrappa"],
      "talk": "For Developers | 101: Getting started with GraphQL and Distributed SQL",
      "time": "10.30 - 11 am",
      "start": "2020-09-15T10:30:00-07:00",
      "end": "2020-09-15T11:00:00-07:00",
      "description": "This GraphQL workshop explores the use of Distributed SQL database for GraphQL workloads. In this workshop, We\"ll walk you the basics of running GraphQL queries and mutation using Hasura GraphQL engine with YugabyteDB. We will also demo a React application for simulating a realtime vehicle tracking backed by Hasura GraphQL engine and YugabyteDB deployed on Kubernetes."
    },
    {
      "id": "i8r8vw",
      "speakers": ["AllisonKunz"],
      "talk": "For Developers | 301: Advanced GraphQL + Distributed SQL Topics",
      "time": "11 - 11.30 am",
      "start": "2020-09-15T11:00:00-07:00",
      "end": "2020-09-15T11:30:00-07:00",
      "description": "This workshop looks at the combination of Hasura Cloud and Yugabyte for production-ready enterprise GraphQL services. We'll dive into the details of how Hasura works under the hood to provide blazing fast real-time APIs at scale plus cell-level role-based security. Then, we'll look at our options for bringing in additional data sources and business logic for a complete enterprise-grade data service."
    },
    {
      "id": "u9wkps",
      "speakers": ["NikhilChandrappa"],
      "talk": "For Developers | Everything Spring Developers Need to Know about Working with Distributed SQL",
      "time": "11.30 am - 12 pm",
      "start": "2020-09-15T11:30:00-07:00",
      "end": "2020-09-15T12:00:00-07:00",
      "description": "In this workshop, we will walk developers through the fundamentals of getting started with Spring Data JPA for accessing relational data in the YugabyteDB cluster. We\"ll be developing a Spring Boot REST application using Spring Data JPA and hibernate ORM for creating Tables and accessing data from the YugabyteDB cluster. And with few configurations changes how developers can switch a Spring Boot application using PostgreSQL database to use YugabyteDB cluster."
    },
    {
      "id": "p4s49m",
      "speakers": ["AlanCaldera"],
      "talk": "For DBAs and Operators | Distributed SQL Security, Encryption, and More",
      "time": "12 - 12.30 pm",
      "start": "2020-09-15T12:00:00-07:00",
      "end": "2020-09-15T12:30:00-07:00",
      "description": "In this workshop, we will walk DBAs and operators through security features of YugabyteDB. Topics covered include:\n<ul>\n<li>Authentication methods</li>\n<li>Encryption at rest</li>\n<li>Encryption on the wire (SSL/TLS)</li>\n</ul>"
    },
    {
      "id": "5uoky0",
      "speakers": ["NikhilChandrappa", "TylerRamer"],
      "talk": "For DBAs and Operators | Yugabyte DB Operator Best Practices",
      "time": "12.30 - 1.30 pm",
      "start": "2020-09-15T12:30:00-07:00",
      "end": "2020-09-15T13:30:00-07:00",
      "description": "In this session, Nikhil Chandrappa and Tyler Ramer from Yugabyte, walks through an overview of YugabyteDB's architecture and how to apply operational best practices to its administrations. Topics covered included:\n<ul>\n<li>Self-service deployments</li>\n<li>Deployment topologies</li>\n<li>Automation</li>\n<li>Data recoverability</li>\n<li>Config management</li>\n<li>Monitoring & health checks</li>\n<li>Security integration</li>\n</ul>"
    },
    {
      "id": "ydbl3r",
      "speakers": ["AlanCaldera"],
      "talk": "Migrating from PostgreSQL to YugabyteDB",
      "time": "1.30 - 2 pm",
      "start": "2020-09-15T13:30:00-07:00",
      "end": "2020-09-15T14:00:00-07:00",
      "description": "In this talk, developers will learn strategies for migrating PostgreSQL to YugabyteDB. We will cover datatype strategies, dealing with serial and sequence numbers, and when to use colocated versus distributed tables."
    },
    {
      "id": "vc24dm",
      "speakers": ["AlanCaldera"],
      "talk": "Migrating from Oracle to YugabyteDB",
      "time": "2 - 2.30 pm",
      "start": "2020-09-15T14:00:00-07:00",
      "end": "2020-09-15T14:30:00-07:00",
      "description": "In this session, we will cover some tricks and tools to migrate Oracle to YugabyteDB. Topics covered will include schema migration, data type mapping, migration tools, and other scenarios."
    },
    {
      "id": "uf8d9d",
      "speakers": ["MikeDenman"],
      "talk": "Migrating from NoSQL to YugabyteDB",
      "time": "2.30 - 3 pm",
      "start": "2020-09-15T14:30:00-07:00",
      "end": "2020-09-15T15:00:00-07:00",
      "description": "In this talk you will learn the various options for migrating NoSQL databases to YugabyteDB incuding lift and shift as well as live migrations with no downtime. Included will be a discussion of the various application design considerations when changing databases."
    }
  ],
  [
    {
      "id": "zi5xav",
      "speakers": ["JoeBeda"],
      "talk": "Kubernetes as a Universal Control Plane",
      "time": "10 - 10.30 am",
      "start": "2020-09-16T10:00:00-07:00",
      "end": "2020-09-16T10:30:00-07:00",
      "description": "In this talk Joe will cover the origins of Kubernetes, its core architecture and how it has evolved beyond containers. Kubernetes is now a distributed systems kernel that is part API gateway, part database and part distributed systems tool kit. We’ll dig into how this applies to the similar evolution of databases into the distributed SQL world."
    },
    {
      "id": "djhitl",
      "speakers": ["MehrdadNurolahzade"],
      "talk": "The Distributed Database Behind Twitter",
      "time": "10.30 - 11 am",
      "start": "2020-09-16T10:30:00-07:00",
      "end": "2020-09-16T11:00:00-07:00",
      "video": "https://vimeo.com/video/457929518",
      "description": "Twitter is giving hundreds of millions of people around the world the power to create and share ideas and information instantly without barriers. Operating such a service requires a global database system that can serve millions of queries per second, with extremely low latency in a real-time environment. In this talk we will look at Twitter's distributed database journey from MySQL to Cassandra to Manhattan; what led Twitter to build its own NoSQL database to meet the unique requirements of serving the public conversations; and what challenges are reshaping the future of distributed databases at Twitter.",
      "transcription": `Hello everyone. My name is Mehrdad Nurolahzade and I'm a member of the real time infrastructure team at Twitter. Today. I'm going to talk to you about the distributed database behind Twitter. I will take you through the journey of Twitter from MySQL to Cassandra, and then from Cassandra to building its own distributed database called Manhattan. The concept of Twitter started in 2006 with this Jack Dorsey tweet. Like many other platforms in mid 2000's, it was implemented on top of the open source MySQL database. The architecture at this point is quite simple; it's basically a monolith, which is internally referred to as the monorail. It's Ruby on Rails. At this point. Twitter is one of the largest Ruby on Rails shops, perhaps, or websites in the world. And this is backed by a single MySQL server with a single leader and a single follower. Now, this is a small platform at this point. It is basically powered by eight servers, the entire platform, and it's capable of processing as many as 600 requests per second. Now, in order to reduce the load on MySQL, the team uses Memcache and almost 90% of the reads are served from the cache. At this point in 2012, engineers are running out of disk on the MySQL servers. The number of tweets is exploding and the platform does not have enough disk basically to store all those tweets. Between 2008 and 2009, the rapid rise in popularity of the Twitter website causes engineers to struggle for scalability. Of course, the monolith is a source of pain for scalability, and there are other limitations as well at the storage layer that I'm going to talk about shortly.
      But, what is happening is that Twitter experiences failure during multiple high profile events. And, the infamous fail whale screen becomes part of the Twitter user experience. So what engineers do, of course, in addition to all the optimizations they're doing basically to improve the Ruby on Rails service, what they're doing is basically at the data layer, they're trying to improve the performance of MySQL. So more caching and trying to do all sorts of data denormalization, sharding of course and whatnot, but unfortunately, due to the time-based sharding strategy that they use at this point, they, introduce hot shards to Twitter's architecture, which again becomes a scalability bottleneck. By 2010, what happens is that engineers have started basically questioning the choice of Ruby on Rails for Twitter. They have started basically investing in Java virtual machine at this point.
      
      Some of the new components of Twitter at this point are implemented in Scala or in JVM in general. One of these components is gizzard. Gizzard is this sharding framework or library, which basically is developed in Scala. And it allows basically, you to use range sharding on top of any storage engine like MySQL. So what is happening is that Twitter has moved its main data sources, including tweets, users, and a social graph to sharded MySQLs basically empowered by gizzard. At the same time, the gizzard library is used to develop an efficient graph database called flockDB. And this is basically to support a user's social graph. Reddis is also being used; the timeline is now being served out of Reddis. In addition, Twitter has started exploring Cassandra.
      Now, Cassandra is interesting to Twitter because of its horizontal scalability and of course, Twitter has plans to basically utilize it for both new use cases, and also old use cases. The idea is that even tweets is going to be moved to Cassandra soon. And finally in 2010, Twitter is building its first data center. Up to this point, Twitter has been using managed hosting. Now 2011 is the year of the great migration for Twitter. Lots of great improvement to Twitter's scalability and availability happens in 2011. First of all, Twitter starts basically abandoning Ruby on Rails and moving towards a service oriented architecture built on top of JVM. And then, it starts developing, Finagle. Finagle framework is now, the RPC framework is built now, to basically support the communication between this element of this service oriented architecture that Twitter is building.
      And the second data center of Twitter comes up and of course it increases the availability of Twitter significantly. Now, this is the result of this migration in 2012. As you can see now, Twitter has this proper multilayered, service oriented architecture with multiple microservices. These microservices at this point, are running under Apache Mesos, which is again, developed at Twitter. But unfortunately even at this point, the main data sources of Twitter – tweets, users, and social graph – are still in MySQL and not in Cassandra, which I'm going to talk about why. Now Twitter's Cassandra adoption is interesting. As I mentioned, we started looking at Cassandra around 2010. This is the point of time that Cassandra is in version 0.5, I believe. And, Twitter is desperate at this point to increase the throughput of request processing.
      And Cassandra is promising because it's horizontally scalable, unlike MySQL. Now, Twitter starts basically using Cassandra for many use cases. By 2012, 2013ish, it has been used for almost 20 use cases. I guess the largest cluster at this point is roughly 200 nodes. And Twitter, of course, lacks Cassandra experience at this point, so it starts hiring engineers with Cassandra expertise, and engineers start basically to engage with open source community. Twitter starts engaging with other companies behind Cassandra, namely Facebook. And they're trying to understand basically the roadmap for the Cassandra project and also starts contributing multiple patches to the Cassandra project. And it's not just Cassandra at this point, it's other basically distributed database solutions as well, like HBase, for example. So Twitter is very active in this case because, it believes Cassandra is going to be the future of database.
      ...the database of choice at Twitter. Now, but unfortunately things do not turn out the way Twitter engineers anticipated. Soon they encounter pain, or roadblocks onboarding some new use cases to Cassandra. The first basically source of pain is Cassandra's gossip protocol. Twitter has some very large data sources like tweets and engineers encounter many, many issues trying to understand and then improve the scalability of gossip protocol beyond a few hundred nodes basically. And then beyond that, engineers noticed that some of the main features they're looking for do not exist in Cassandra. For example, counter support that the observability customer at Twitter needed did not exist. And finally, after two years of managing Cassandra clusters, the storage team at Twitter basically notices that a significant amount of their bandwidth is spent managing Cassandra.
      And this is primarily because Cassandra lacks automation in its ecosystem at this point. So engineers start to question suitability of Cassandra and start thinking that maybe satisfying the unique requirements of various use cases at Twitter would be more cost effective if they developed their own custom in house database solution. Now that's not the entire story. There is another dimension to this story and that's Twitter's Lambda architecture. So around 2012, before those extreme processing platforms are in place, Lambda architecture is very popular for processing large data sources under Hadoop. Twitter needs something to serve the outcome of its batching layer in its Lambda architecture. They look at Memcache, Redis, and Cassandra, and each of these solutions are not suitable due to various reasons because making them meet the requirements or integrate them with Hadoop is considered to be non-trivial.
      So they eventually think of building something in house, a read-only NoSQL system, and this is called Manhattan. And this is basically developed very fast, just a matter of a couple of months, maybe. Now the speed at which they develop this solution, makes them think that, hey, maybe if we build something in house specific to the requirements of Twitter, it's going to be much faster and much more efficient compared to what experience they had with Cassandra. And this is basically the point of time the priorities or the goals shift from making Cassandra work to now making something scalable, low-latency, with minimal operational overhead that can replace Cassandra. So the work is starts in 2012 and by 2014, pretty much all Cassandra use cases and many of the MySQL use cases are now migrated to Manhattan.
      Now Manhattan was ideas collected from multiple basically storage systems that Twitter engineers played around with. So they kind of actually designed it based on these principles. So they want it to be massively scalable. It should scale to thousands of nodes per cluster, and it should be highly available at the cost of low consistency. And for the sake of resolving these inconsistencies between data, it should basically use a last write wins strategy for conflict resolution. Now, given that Twitter has both read heavy and write heavy workloads, it should be efficient to satisfy both of these types of workloads. And it should satisfy both multitenant and multi-region type of clusters and different types of storage engines that are suitable for different types of workloads. It should be flexible and be schemaless and use random partitioning and minimize operations through automated operation and self service for their customers.
      The data model. Sorry, before that, let me talk about the tech stack. So primarily developed in Java with some elements in Scala. Python was also used for the sake of automation and tooling. Of course, finagle and Netty are used for the sake of communication between the client and server. Zookeeper is there for the sake of topology and currently, Manhattan is running outside containers, but we're in the process of migrating to Kubernetes. The data model is quite simple. It's just basically, every record has a partition key, a local key, which is optional, and a value, again, optional. And these components are scalar types, like strings or numbers or binaries even, or they can be composite types like, can be a combination of multiple scalar types. And the partition key decides which partition out of many thousands in the cluster this key belongs to. And hash function is used basically for the mapping of partition keys to partitions. And like I said, cluster can have thousands of partitions. And this mapping between partition can know this stored under topology in Zookeeper. Now similar to the 2007 paper, the Amazon dynamo paper, Manhattan's architecture is leaderless replication, which means that the stateless frontend coordinator replicates the request to all the replicas in the cluster and basically gathers responses from them. Now, if there is any conflict in the response that is returned by these replicas, the last write wins strategy is used, basically. So whatever timestamp is the highest, basically becomes the value that is returned to the client. Now, in terms of consistency, the coordinator waits until it receives a required number of responses from replicas and this is basically tuneable per request. So a request can have a consistency level of any, one, zero or all or quorum within a single region. And then the coordinator is going to wait until basically these number of requests are returned. Now, given that at Twitter, we use quorum reads and writes by default, this basically guarantees your own write consistency for the majority of our use cases.
      Of course, this is eventual consistency and in eventual consistency, inconsistency can happen. And similar to Cassandra, we you have read repair and anti entropy repairs for repairing the inconsistent state of data. In terms of storage engines, we have multiple of them, but the four that have been used more than any other ones are SeaDB, which is the very first one developed for the sake of the Lambda architecture. This is a read-only backend, basically very efficient in terms of basically look-ups, in memory or on disk. Also we have the SSTable backend, which is borrowed from the Bigtable, Cassandra also uses it and it's very efficient for the write heavy workloads. We developed the B-Tree based backend around 2016. This is basically based on MySQL's InnoDB and this was suited for a read heavy type of workload that had sort of lighter right workloads.
      And then RocksDB has become basically the dominant type of backend in Manhattan clusters. As of 2019, we use them basically for both read heavy and write heavy workloads. I mentioned Manhattan initially was designed to be eventual consistency, but we realized the fact that some of our customers are interested in strong consistency. So what we did later on was we extended the architecture of Manhattan and plugged this sort of replicated log solution, a Kafka-like solution, into its request processing path. So what is happening is that, uh, incoming requests are written to a shard of this replicated log based on the partition key. And these replicas on the other hand are guaranteed to see these incoming and requests in the exact same order. So this kind of actually provides a linearizable consistency on a partition key basis for the requests. We have local strong consistency within one region and we have global strong consistency within all regions. And at the same time, we allow these eventual type of reads, which are faster against this strong consistency data. Of course, due to the fact that the replicated log now sits between basically the request processing path, the end-to-end latency of these operations is almost an order of magnitude slower than the eventual consistency type of operation.
      Quickly talk about secondary indexing at Manhattan. It's similar to DynamoDB. We have both local secondary indexes and global secondary indexes. I'm going to talk about the differences shortly, but, uh, secondary indexes can be defined on the components of the local key and value. Now the global secondary index is one in which basically the primary key or the partition key of the index can be different from that of the master record. So in a sense, uh, the global, uh, secondary index can be used to read from multiple partitions. On the contrary, the local secondary index, uh, the partition key of the index and the master record needs to be the same. So in a local secondary index, you can only read from one partition at a time.
      And finally, we have also support for automated imports and exports in Manhattan. So what is happening is primarily with Hadoop, we have [integration to] for our read only clusters basically you can automatically import data from Hadoop; and for our read-write clusters, basically you can export data to Hadoop. So Hadoop is, of course, it's used in this architecture for the sake of various analytics aggregations, or, any sort of batch processing that you need. We're currently in the process of integrating this architecture with public cloud storage, as well, so soon our customers are going to be able to import data from public cloud or export it to public cloud and use these third-party integrations available to do various analytics.
      About the scale of Manhattan, we are at this point operating, I believe, close to 20 production clusters, and, um, probably we have more than a thousand customers. We definitely have more, more than a thousand databases in Manhattan. Um, and these are powered by tens of thousands of nodes. I believe the largest cluster we have at this point is over 4,000 nodes. And in terms of the size of the data, um, we are serving many petabytes of data in Manhattan, and this is, uh, served at the rate of probably tens of millions of requests per second. Now let's talk about the fun thing: operations.
      Well
      Operations are easy at a smaller scale. Um, so if you have operated small storage clusters, you, you already know that, you know, every once in a while, basically, maybe one of your nodes dies and then you have to replace it with a new node. Every once in a while, maybe you have to add capacity. Maybe you have to update the cluster. So because the set of operations or the space of operations that you have to do is basically relatively limited. And, um, you can easily reason about basically what to do or in what order to do, so potentially you're going to be doing this, uh, sequentially, maybe first remove the node then add a replacement for it, and then maybe update, uh, upgrade the cluster. So this is easy, but what if you are operating a cluster at the size of Manhattan? So you operating clusters at the size of hundreds of nodes or even thousands of nodes.
      So, because the cluster at this scale is so large, so is the space of things that can happen together. So you can have at any given point of time, many of these nodes failing. So an operator might be basically, uh, you know, acting on removing multiple of these nodes. And it could be at the same time that one operator is taking care of these bad nodes, there is a surge in traffic, and all of a sudden another operator is tasked to add extra emergency capacity to this cluster, uh, to basically, uh, allow serving, uh, this surge of traffic. And to make the
      Matter, even worse.
      
      Uh, it could be that, um, another operator and the team is basically, uh, trying to update the cluster at the same time. And these operations, um, the all can happen at the same time by different people. The question is like, how do you, how do you coordinate between these operations and these people? Um, like how do they know if an operation is safe to do at any given point of time, or in what order it has to be done, or how long should they babysit the cluster until the operation is complete? So in summary, doing operations safely and correctly in scalable clusters is very, very difficult. So for that matter, we developed, Genie. Genie is the name of our, um, Manhattan cluster manager. It's a service basically which has complete understanding of, uh, the cluster. For example, it knows, uh, the state of every single node, whether it's restarting or being removed or being added to topology, or is failing at this point. So whenever, uh, operators want to do something, they communicate their intent to Genie and then Genie, it knows exactly if the operation is, uh, safe to do at this point. So if it's safe to do is just enqueues that operation and tries to execute it with all the other operations that Genie is executing at this point. So this way, by pushing this knowledge and this basically burden of doing this to automation,
      Manhattan operators,
      Do not need to have you know that much of domain knowledge about how to do these operations, nor that they need to basically babysit Manhattan clusters while these operations are being executed.
      Now fast forward to 2020,
      What are we working on? So of course, uh, data protection compliance is top priority at Twitter. And I'm assuming in many other companies. We've been working very hard over the past few years, making sure that Manhattan is compliant via these regulations. Now I mentioned about RocksDB migration, we're finalizing migration to the last remaining clusters at Twitter. And Kubernetes is ongoing. We're basically porting Manhattan under Kubernetes. We're rebuilding our cross DC or cross region replication feature on top of Kafka. And as I mentioned, we are integrating public cloud storage into our, um, import and export pipelines. Now looking forward beyond 2020, um, we still believe Manhattan is the right solution for many use cases at Twitter. After all Manhattan was custom built to, uh, meet the official requirements of these, uh, use cases. Uh, so we're gonna be continuing to build on top of Manhattan.
      However, um, the diversity of use cases in a company at a scale of Twitter cannot really be efficiently served by a single database. Um, so we believe, um, if our customers try to, uh, use Manhattan for use cases, where it wasn't designed for then they're going to basically be facing inefficiency because of various work arounds and, um, you know, tradeoffs that they're going to be making. At the same time, if we try to extend Manhattan so that it satisfied every single use case, it's going to result in inefficiency of Manhattan at the same time. So our position is that we try to, uh, thoughtfully and logically evolve Manhattan, but at the same time, we want to be able to provide the best, uh, storage solution to Twitter. So what does that mean? Well, looking at the distributed database landscape, we see that it has significantly evolved since 2012, which [is when] we decided to build Manhattan.
      So multiple new open source solutions have basically popped up. And some of these are backed by companies that are operating even at the scale of Twitter, right. And existing solutions like Cassandra have evolved a lot. At the same time, these companies like Amazon, Microsoft, and Google, have started basically offering their internal databases as managed solutions. So at Twitter, we hypothesize that some of these solutions can potentially complement or maybe even replace our internal offerings in the future. Well, that's all I have. Um, but before I wrap it up, I just want to share with you a bunch of resources. So over the years we have, um, published a few blog posts and video presentations about Manhattan. So do check them out if you're interested to learn more about Manhattan. So there is one on strong consistency, uh, automated operations in Manhattan. And, uh, a look at secondary indexing in Manhattan. Thank you very much.`
    },
    {
      "id": "rgjt2s",
      "speakers": ["BreneshStanslasFlowerMary"],
      "talk": "Monolith to Microservices",
      "time": "11 - 11.30 am",
      "start": "2020-09-16T11:00:00-07:00",
      "end": "2020-09-16T11:30:00-07:00",
      "description": "How a large intertwined Mainframe Monolith can be broken down into Microservices in a practical approach. The end-to-end approach covers Wipro’s proprietary “moderniZ” platform which addresses UI, Services and Data Transformation."
    },
    {
      "id": "p8mjfa",
      "speakers": ["HaleDonertasli"],
      "talk": "5G and Data Challenges",      
      "time": "11.30 am - 12 pm",
      "start": "2020-09-16T11:30:00-07:00",
      "end": "2020-09-16T12:00:00-07:00",
      "description": "5G demands full capacity and availability of data delivery while living in a decade that all layers of connectivity are going through the massive partitioning. In this session, we are going to discuss the strategies to deal with data challenges for 5G."
    },
    {
      "id": "s7zi2b",
      "speakers": ["LianghongXu"],
      "talk": "Pinterest’s Exploration of Distributed SQL",
      "time": "12 - 12.30 pm",
      "start": "2020-09-16T12:00:00-07:00",
      "end": "2020-09-16T12:30:00-07:00",
      "description": "Pinterest hosts one of the largest industry deployments of HBase. A number of important storage services were built on top of it, including the in-house key-value store and graph database, serving a huge amount of business critical data. Over the past few years, we have seen an increasing number of customer requests for distributed storage that offers SQL-like features such as distributed transactions and secondary indexing, while achieving scalability of a NoSQL database. To address these requirements, we built dedicated middleware on top of HBase that provide these additional functionalities. This talk presents the evolution of Pinterest’s online storage services (with a focus around HBase), the challenges we face today and our early exploration of a distributed SQL database."
    },
    {
      "id": "0wayh3",
      "speakers": ["AllisonKunz"],
      "talk": "Modernizing Application Development with Planet-scale GraphQL & Distributed SQL",
      "time": "12.30 - 1 pm",
      "start": "2020-09-16T12:30:00-07:00",
      "end": "2020-09-16T13:00:00-07:00",
      "video": "https://player.vimeo.com/video/460327414",
      "description": "Cloud native patterns allow developers to store data and run their compute on global infrastructure, distributed just like their customers. But once you've solved the distributed data problem, how do you serve that data scalably, reliably, and with cell-level security? How do you bring in the many other data sources your applications require, both internal (another division) and external (a third-party REST API, for example)? We'll look at how Hasura provides a robust, performant, and cost-effective modern API layer with a seamless developer experience."
    },
    {
      "id": "fuqmyr",
      "speakers": ["JamesHartig"],
      "talk": "How Admiral Scales Globally with YugabyteDB on Google Cloud While Maintaining Single-Digit Latency",
      "time": "1 - 1.30 pm",
      "start": "2020-09-16T13:00:00-07:00",
      "end": "2020-09-16T13:30:00-07:00",
      "description": "What if you’re a small company running a SaaS application in the cloud with millions of end users, and you need to scale globally at single-digit latency? Admiral helps online publishers engage with visitors through adblock recovery, paid subscriptions, privacy and consent management, and more. Our dataset is large and complex. The previous NoSQL database couldn’t scale, so we moved to distributed SQL. Our Go application runs in Google Cloud across 5 regions in 3 continents. This geo-distributed architecture is powered by a single YugabyteDB cluster that delivers an average global read latency of 3ms! In this talk, we’ll look at how distributed SQL can be deployed on modern cloud infrastructure to provide a scalable, resilient, low latency, fully consistent data service."
    },
    {
      "id": "14mbp3",
      "speakers": ["ChristophPakulski", "PrasadRadhakrishnan"],
      "talk": "Envoy & Service Mesh for Databases: What the Future Holds",
      "time": "1.30 - 2.30 pm",
      "start": "2020-09-16T13:30:00-07:00",
      "end": "2020-09-16T14:30:00-07:00",
      "description": "Service Mesh is a concept and technology that helps with creating, managing, securing, monitoring the network of apps/microservices and the interactions between them. It's new but fairly understood and implemented in all modern cloud-native architectures, platforms and deployments. Is it applicable in the context of App to Database interactions? Postgres proxy filter in Envoy is a significant step in that direction. Prasad from Yugabyte and Christoph from Tetrate are co-presenting their views on this subject."
    },
    {
      "id": "nn0u9p",
      "speakers": ["TobiasMeixner"],
      "talk": "A Migration Journey from Amazon DynamoDB to Yugabyte YSQL and Hasura",
      "time": "2.30 - 3 pm",
      "start": "2020-09-16T14:30:00-07:00",
      "end": "2020-09-16T15:00:00-07:00",
      "description": "Switching databases is painful, even more so going from NoSQL to SQL. This talk will give insights into BRIKL's migration path from Apollo Server and Amazon DynamoDB over to Hasura and Yugabyte YSQL, going from a growing monolith to a scalable service-oriented architecture."
    }
  ],
  [
    {
      "id": "hohbbp",
      "speakers": ["JoeHellerstein"],
      "talk": "The Art of the State: Serverless Computing and Distributed Data",
      "video": "https://player.vimeo.com/video/459492516",
      "time": "10 - 10.30 am",
      "start": "2020-09-17T10:00:00-07:00",
      "end": "2020-09-17T10:30:00-07:00",
      "description": "Serverless computing is a first step to opening up the cloud as a programmable platform for developers. As always, the hardest and most interesting part is the data, a topic that is poorly served by first-generation serverless platforms. In this talk I’ll go over the challenges of data management in serverless computing, and promising new results from the Anna KVS and Cloudburst “stateful” serverless computing efforts in Berkeley’s RISELab."
    },
    {
      "id": "6x6zbe",
      "speakers": ["PuneetDevadiga", "HaleDonertasli", "PrasadRadhakrishnan", "KartikRallapalli", "JamesTaylor"],
      "talk": "The Future of Data Infrastructure, A Telco Roundtable",
      "time": "10.30 - 11.30 am",
      "start": "2020-09-17T10:30:00-07:00",
      "end": "2020-09-17T11:30:00-07:00",
      "description": "Join this roundtable with technology leaders and innovators from some of the world's most loved enterprises in the telco industry, and explore the future of data infrastructure. A panel of architects will get together to discuss cloud native trends and challenges in the context of Digital, IoT, 5G, Edge, Data and how it fuels their need to innovate."
    },
    {
      "id": "2i7n91",
      "speakers": ["MaheshTyagarajan"],
      "talk": "Transforming the Omni-Channel Experience at Kroger",
      "time": "11.30 am - 12 pm",
      "start": "2020-09-17T11:30:00-07:00",
      "end": "2020-09-17T12:00:00-07:00",
      "description": "We are evolving the technology stack at Kroger to meet the transformational needs of the business – to become a true omni-channel retailer in the food/grocery space. The technology stack evolves to a cloud native microservices platform which moves from an on-prem ecosystem to a hybrid of on-prem and public cloud infrastructure. Along this journey we are not lifting and shifting but rethinking the entirety of the stack to modern cloud agnostic, cloud native architectures by using an abundance of open source (and COSS), highly scaled shared-nothing components, all based on the foundation of Kubernetes."
    },
    {
      "id": "2nrj9y",
      "speakers": ["HudsonClark", "EthanUberseder"],
      "talk": "Scaling to Billions of Internet Observations per Day",
      "time": "12 - 12.30 pm",
      "start": "2020-09-17T12:00:00-07:00",
      "end": "2020-09-17T12:30:00-07:00",
      "description": "A cohesive and queryable model of every device on the Internet lies at the heart of Censys’ products. In order to improve the security of individuals, companies, and government organizations with this data, Censys ingests and processes billions of network handshakes daily. To support the growth of the company, the datastore that backs this model must be resilient to failure and horizontally scalable, while still supporting the more traditional RDBS features demanded by the API. This talk will discuss the unique business and engineering requirements of this particular database system, and how Censys deployed YugabyteDB successfully to meet these demands."
    },
    {
      "id": "ct3gwj",
      "talk": "Cloud Native Spring for Relational Databases",
      "speakers": ["DaShaunCarter"],
      "time": "12.30 - 1 pm",
      "start": "2020-09-17T12:30:00-07:00",
      "end": "2020-09-17T13:00:00-07:00",
      "description": "Traditionally spring framework has been popular with accessing RDBMS databases for its compressive transaction support and declarative way of executing transactions compared to EJBs. As enterprises embrace cloud-native architecture, Spring Data projects have been very popular for implementing data access APIs in spring microservices, partly due to the consistent abstractions provided by spring data irrespective of SQL dialects. In this talk, DaShaun will walk you through current state of Spring Data support for building scalable and resilient cloud native microservices using modern relational databases."
    },
    {
      "id": "9c23yp",
      "speakers": ["TravisLogan"],
      "talk": "Evolve: A Database Journey from Ground to Cloud",
      "time": "1 - 1.30 pm",
      "start": "2020-09-17T13:00:00-07:00",
      "end": "2020-09-17T13:30:00-07:00",
      "description": "With over 14 years of experience with MS SQL Server, Travis is well-versed in database evolution. This talk will start with traditional RDBMS experiences and will lead us through redundancy, scaling, performance and many other trials and tribulations along the way. By the end of the talk, you’ll have explored Travis’ complete evolution from traditional database systems of the past and important milestones that led him to the distributed SQL systems of today."
    },
    {
      "id": "pxg2y3",
      "speakers": ["KenOwens"],
      "talk": "The Data Divide: An End User’s Cloud Native Journey to Distributed Databases",
      "time": "1.30 - 2 pm",
      "start": "2020-09-17T13:30:00-07:00",
      "end": "2020-09-17T14:00:00-07:00",
      "description": "The modern cloud native developer is torn today when it comes to managing distributed databases in the platforms or cloud services due to complexity, technology gaps, and data aspects impacted but cloud native principles. This talk will describe the journey Mastercard has undergone, the successes and lessons learned. The attendee will obtain a better understanding about the ever changing opensource and enterprise vendors adoption of cloud native principles and this impact on the strategy and direction to achieve the business objectives."
    },
    {
      "id": "pldncj",
      "speakers": ["EvanPowell"],
      "talk": "Container Attached Storage: Open Source Extensions to Kubernetes for Data",
      "time": "2 - 2.30 pm",
      "video": "https://player.vimeo.com/video/459492603",
      "start": "2020-09-17T14:00:00-07:00",
      "end": "2020-09-17T14:30:00-07:00",
      "description": "OpenEBS is an active CNCF project that helps in the use of Kubernetes for running databases and other stateful workloads. How? What is next?"
    },
    {
      "id": "y3hijh",
      "speakers": ["KarthikRanganathan"],
      "talk": "Closing Keynote: Distributed SQL Virtual Summit 2020",
      "time": "2.30 - 3 pm",
      "start": "2020-09-17T14:30:00-07:00",
      "end": "2020-09-17T15:00:00-07:00",
      "description": "Closing remarks"
    }
  ]
]